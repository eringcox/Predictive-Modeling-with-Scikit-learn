{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agenda**\n",
    "\n",
    "- What is preprocessing and how it is done?\n",
    "\n",
    "We will learn how to do the following:\n",
    "1. Train/Test split\n",
    "2. Normalization/Standardization\n",
    "3. Label Encoding\n",
    "4. Handling Missing Data\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing overview\n",
    "\n",
    "In simple words, pre-processing refers to the transformations applied to your data before feeding it to the algorithm \n",
    "\n",
    "Scikit-learn library has a pre-built functionality under **sklearn.preprocessing** that we will explore in this module\n",
    "\n",
    "## Train-Test split\n",
    "\n",
    "Setting portion of data aside - prerequisite for generalization\n",
    "\n",
    "Different partitions for training and testing - A core practice in machine learning\n",
    "\n",
    "Scikit-learn has a convenient method to assist in that process:\n",
    "\n",
    "**train_test_split(sample, response, test_size=0.25, shuffle=True)**\n",
    "\n",
    "The split size is controlled using the **attribute test_size** \n",
    "\n",
    "**Default test_size** - 25% of the dataset size\n",
    "\n",
    "**Standard practice** - shuffle the dataset before splitting by setting the attribute **shuffle=True**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split in train and test sets\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, shuffle=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Scale or Not To Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is a monotonic transformation \n",
    "* The relative order of smaller to larger value in a variable is maintained post the scaling\n",
    "\n",
    "Vast majority of algorithms require scaling\n",
    "\n",
    "Algorithms that do not require scaling:\n",
    "\n",
    "> Algorithms that rely on rules \n",
    "* They would not be affected by any monotonic transformations of the variables\n",
    "* e.g. CART, Random Forests, Gradient Boosted Decision Trees\n",
    "\n",
    "> Algorithms that rely on distributions of the variables\n",
    "* e.g. Naive Bayes\n",
    "\n",
    "[Standardization and Normalization](https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization) are ways to do scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms the features into a Standard Gaussian (or normal) distribution with a mean of 0 and standard deviation of 1\n",
    "\n",
    "Used when algorithm requires computation of distance (Euclidean) to avoid large scale features dominating others \n",
    "* e.g. KNN, K-means, Minimum distance classifier\n",
    "\n",
    "It matters in PCA to avoid bias towards high magnitude features \n",
    "\n",
    "For gradient descent based algorithm standardization helps in faster convergence\n",
    "\n",
    "In SVM it can reduce the time to find support vectors\n",
    "\n",
    "Scikit-learn implements data standardization in the StandardScaler module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do standardization\n",
    "# recall standardization has mean 0, but can still have < -1 and > 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "standardize_Xtrain = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.1, 2.9, 4.7, 1.4],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [5.7, 2.6, 3.5, 1. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall data is shuffled\n",
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34791671, -0.40297225,  0.57605563,  0.32985562],\n",
       "       [ 0.22838704, -2.07456085,  0.17779495, -0.20171027],\n",
       "       [ 1.06509472,  0.07462449,  0.40537248,  0.32985562],\n",
       "       [ 1.30415405,  0.07462449,  0.97431631,  1.26009593],\n",
       "       [-0.13020196, -1.11936736, -0.10667697, -0.20171027]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardize_Xtrain[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Standardize X_test and print how first 5 rows of X_test and standardize_Xtest look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.5, 1.3, 0.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 4. , 1.2, 0.2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize X_test\n",
    "\n",
    "# recall that scaler was fit to X_train, so no need to re-fit the test \n",
    "# (don't know what the test data is, so need to use what X_train was)\n",
    "standardize_X_test = scaler.transform(X_test)\n",
    "\n",
    "X_test[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96690963,  1.02981797, -1.3583534 , -1.13195058],\n",
       "       [ 0.46744638, -0.40297225,  0.3484781 ,  0.19696415],\n",
       "       [ 0.58697605, -1.35816573,  0.74673878,  0.99431299],\n",
       "       [ 1.30415405,  0.07462449,  0.80363316,  1.52587888],\n",
       "       [-0.01067229,  2.22380983, -1.41524778, -1.26484206]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardize_X_test[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization transforms the features in the dataset so that it has a unit norm or has magnitude or length of 1 \n",
    "\n",
    "The length of a vector is the square-root of the sum of squares of the vector elements \n",
    "\n",
    "A unit vector (or unit norm) is obtained by dividing the vector by its length \n",
    "\n",
    "Note: Normalizing the dataset is particularly useful in scenarios where the dataset is sparse (i.e., a large number of observations are zeros) and also have differing scales \n",
    "\n",
    "Normalization in Scikit-learn is implemented in the Normalizer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "normalize_Xtrain = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.1, 2.9, 4.7, 1.4],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [5.7, 2.6, 3.5, 1. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73081412, 0.34743622, 0.56308629, 0.16772783],\n",
       "       [0.78892752, 0.28927343, 0.52595168, 0.13148792],\n",
       "       [0.76945444, 0.35601624, 0.50531337, 0.16078153],\n",
       "       [0.72415258, 0.32534391, 0.56672811, 0.22039426],\n",
       "       [0.78667474, 0.35883409, 0.48304589, 0.13801311]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_Xtrain[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Normalize X_test and print how first 5 rows of X_test and normalize_Xtest look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize X_test\n",
    "\n",
    "# again, already fit with xtrain\n",
    "normalize_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.5, 1.3, 0.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 4. , 1.2, 0.2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80033301, 0.56023311, 0.20808658, 0.04801998],\n",
       "       [0.75728103, 0.3542121 , 0.52521104, 0.15878473],\n",
       "       [0.72965359, 0.28954508, 0.57909015, 0.22005426],\n",
       "       [0.73337886, 0.32948905, 0.54206264, 0.24445962],\n",
       "       [0.81120865, 0.55945424, 0.16783627, 0.02797271]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_X_test[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Recall only use Standardization OR Normalization, not both\n",
    "\n",
    "if values are between 0 and 1, use normalization, if guassian dist with mean 0, use standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding or Encoding Categorical Variables\n",
    "\n",
    "Encoding - converting non-numerical features with labels into a numerical representation\n",
    "\n",
    "Encoding in Scikit-learn using LabelEncoder - for encoding labels as integers \n",
    "\n",
    "We have already seen how species were encoded in iris dataset:\n",
    "* 0 = setosa, 1 = versicolor, 2 = virginica\n",
    "\n",
    "In avocado dataset:\n",
    "The `region_categories` and `type_categories variables` store the unique categories in the `region` and `type` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region categories are: \n",
      " ['Albany' 'Atlanta' 'BaltimoreWashington' 'Boise' 'Boston'\n",
      " 'BuffaloRochester' 'California' 'Charlotte' 'Chicago' 'CincinnatiDayton'\n",
      " 'Columbus' 'DallasFtWorth' 'Denver' 'Detroit' 'GrandRapids' 'GreatLakes'\n",
      " 'HarrisburgScranton' 'HartfordSpringfield' 'Houston' 'Indianapolis'\n",
      " 'Jacksonville' 'LasVegas' 'LosAngeles' 'Louisville' 'MiamiFtLauderdale'\n",
      " 'Midsouth' 'Nashville' 'NewOrleansMobile' 'NewYork' 'Northeast'\n",
      " 'NorthernNewEngland' 'Orlando' 'Philadelphia' 'PhoenixTucson'\n",
      " 'Pittsburgh' 'Plains' 'Portland' 'RaleighGreensboro' 'RichmondNorfolk'\n",
      " 'Roanoke' 'Sacramento' 'SanDiego' 'SanFrancisco' 'Seattle'\n",
      " 'SouthCarolina' 'SouthCentral' 'Southeast' 'Spokane' 'StLouis' 'Syracuse'\n",
      " 'Tampa' 'TotalUS' 'West' 'WestTexNewMexico'] \n",
      "\n",
      "Type categories are: \n",
      " ['conventional' 'organic'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "avocado_path = 'avocado.csv' #please make sure avacado.csv is in the same directory as the iPython notebook\n",
    "avocado_df = pd.read_csv(avocado_path,header=0)\n",
    "avocado_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "region_categories = avocado_df['region'].unique()\n",
    "type_categories = avocado_df['type'].unique()\n",
    "\n",
    "print('Region categories are: \\n',region_categories,'\\n')\n",
    "print('Type categories are: \\n',type_categories,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoded region categories are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "region_encoder = LabelEncoder()\n",
    "encoded_region_cats = region_encoder.fit_transform(region_categories)\n",
    "print('The encoded region categories are:', encoded_region_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Encoding the region column was: \n",
      " 0                  Albany\n",
      "1                  Albany\n",
      "2                  Albany\n",
      "3                  Albany\n",
      "4                  Albany\n",
      "5                  Albany\n",
      "6                  Albany\n",
      "7                  Albany\n",
      "8                  Albany\n",
      "9                  Albany\n",
      "10                 Albany\n",
      "11                 Albany\n",
      "12                 Albany\n",
      "13                 Albany\n",
      "14                 Albany\n",
      "15                 Albany\n",
      "16                 Albany\n",
      "17                 Albany\n",
      "18                 Albany\n",
      "19                 Albany\n",
      "20                 Albany\n",
      "21                 Albany\n",
      "22                 Albany\n",
      "23                 Albany\n",
      "24                 Albany\n",
      "25                 Albany\n",
      "26                 Albany\n",
      "27                 Albany\n",
      "28                 Albany\n",
      "29                 Albany\n",
      "               ...       \n",
      "18219             TotalUS\n",
      "18220             TotalUS\n",
      "18221             TotalUS\n",
      "18222             TotalUS\n",
      "18223             TotalUS\n",
      "18224             TotalUS\n",
      "18225                West\n",
      "18226                West\n",
      "18227                West\n",
      "18228                West\n",
      "18229                West\n",
      "18230                West\n",
      "18231                West\n",
      "18232                West\n",
      "18233                West\n",
      "18234                West\n",
      "18235                West\n",
      "18236                West\n",
      "18237    WestTexNewMexico\n",
      "18238    WestTexNewMexico\n",
      "18239    WestTexNewMexico\n",
      "18240    WestTexNewMexico\n",
      "18241    WestTexNewMexico\n",
      "18242    WestTexNewMexico\n",
      "18243    WestTexNewMexico\n",
      "18244    WestTexNewMexico\n",
      "18245    WestTexNewMexico\n",
      "18246    WestTexNewMexico\n",
      "18247    WestTexNewMexico\n",
      "18248    WestTexNewMexico\n",
      "Name: region, Length: 18249, dtype: object\n",
      "\n",
      "\n",
      "After Encoding the region column is: \n",
      " 0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "5         0\n",
      "6         0\n",
      "7         0\n",
      "8         0\n",
      "9         0\n",
      "10        0\n",
      "11        0\n",
      "12        0\n",
      "13        0\n",
      "14        0\n",
      "15        0\n",
      "16        0\n",
      "17        0\n",
      "18        0\n",
      "19        0\n",
      "20        0\n",
      "21        0\n",
      "22        0\n",
      "23        0\n",
      "24        0\n",
      "25        0\n",
      "26        0\n",
      "27        0\n",
      "28        0\n",
      "29        0\n",
      "         ..\n",
      "18219    51\n",
      "18220    51\n",
      "18221    51\n",
      "18222    51\n",
      "18223    51\n",
      "18224    51\n",
      "18225    52\n",
      "18226    52\n",
      "18227    52\n",
      "18228    52\n",
      "18229    52\n",
      "18230    52\n",
      "18231    52\n",
      "18232    52\n",
      "18233    52\n",
      "18234    52\n",
      "18235    52\n",
      "18236    52\n",
      "18237    53\n",
      "18238    53\n",
      "18239    53\n",
      "18240    53\n",
      "18241    53\n",
      "18242    53\n",
      "18243    53\n",
      "18244    53\n",
      "18245    53\n",
      "18246    53\n",
      "18247    53\n",
      "18248    53\n",
      "Name: region, Length: 18249, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Before Encoding the region column was: \\n', avocado_df['region'])\n",
    "print('\\n')\n",
    "\n",
    "avocado_df['region'] = region_encoder.transform(avocado_df['region'])\n",
    "print('After Encoding the region column is: \\n', avocado_df['region'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Encode type categories print the encoded type categories and print the column before and after encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoded type categories are: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Encoded Type Categories\n",
    "type_encoder = LabelEncoder()\n",
    "encoded_type_cats = type_encoder.fit_transform(type_categories)\n",
    "print('The encoded type categories are:', encoded_type_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Encoding the type column was: \n",
      " 0        conventional\n",
      "1        conventional\n",
      "2        conventional\n",
      "3        conventional\n",
      "4        conventional\n",
      "5        conventional\n",
      "6        conventional\n",
      "7        conventional\n",
      "8        conventional\n",
      "9        conventional\n",
      "10       conventional\n",
      "11       conventional\n",
      "12       conventional\n",
      "13       conventional\n",
      "14       conventional\n",
      "15       conventional\n",
      "16       conventional\n",
      "17       conventional\n",
      "18       conventional\n",
      "19       conventional\n",
      "20       conventional\n",
      "21       conventional\n",
      "22       conventional\n",
      "23       conventional\n",
      "24       conventional\n",
      "25       conventional\n",
      "26       conventional\n",
      "27       conventional\n",
      "28       conventional\n",
      "29       conventional\n",
      "             ...     \n",
      "18219         organic\n",
      "18220         organic\n",
      "18221         organic\n",
      "18222         organic\n",
      "18223         organic\n",
      "18224         organic\n",
      "18225         organic\n",
      "18226         organic\n",
      "18227         organic\n",
      "18228         organic\n",
      "18229         organic\n",
      "18230         organic\n",
      "18231         organic\n",
      "18232         organic\n",
      "18233         organic\n",
      "18234         organic\n",
      "18235         organic\n",
      "18236         organic\n",
      "18237         organic\n",
      "18238         organic\n",
      "18239         organic\n",
      "18240         organic\n",
      "18241         organic\n",
      "18242         organic\n",
      "18243         organic\n",
      "18244         organic\n",
      "18245         organic\n",
      "18246         organic\n",
      "18247         organic\n",
      "18248         organic\n",
      "Name: type, Length: 18249, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the column before, encode type categories\n",
    "print('Before Encoding the type column was: \\n', avocado_df['type'])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Encoding the type column is: \n",
      " 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "5        0\n",
      "6        0\n",
      "7        0\n",
      "8        0\n",
      "9        0\n",
      "10       0\n",
      "11       0\n",
      "12       0\n",
      "13       0\n",
      "14       0\n",
      "15       0\n",
      "16       0\n",
      "17       0\n",
      "18       0\n",
      "19       0\n",
      "20       0\n",
      "21       0\n",
      "22       0\n",
      "23       0\n",
      "24       0\n",
      "25       0\n",
      "26       0\n",
      "27       0\n",
      "28       0\n",
      "29       0\n",
      "        ..\n",
      "18219    1\n",
      "18220    1\n",
      "18221    1\n",
      "18222    1\n",
      "18223    1\n",
      "18224    1\n",
      "18225    1\n",
      "18226    1\n",
      "18227    1\n",
      "18228    1\n",
      "18229    1\n",
      "18230    1\n",
      "18231    1\n",
      "18232    1\n",
      "18233    1\n",
      "18234    1\n",
      "18235    1\n",
      "18236    1\n",
      "18237    1\n",
      "18238    1\n",
      "18239    1\n",
      "18240    1\n",
      "18241    1\n",
      "18242    1\n",
      "18243    1\n",
      "18244    1\n",
      "18245    1\n",
      "18246    1\n",
      "18247    1\n",
      "18248    1\n",
      "Name: type, Length: 18249, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the column after encoding\n",
    "avocado_df['type'] = type_encoder.transform(avocado_df['type'])\n",
    "print('After Encoding the type column is: \\n', avocado_df['type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Missing Data\n",
    "\n",
    "It is often the case that a dataset contains several missing observations \n",
    "\n",
    "Scikit-learn implements the Imputer module for completing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5677.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveragePrice  Total Volume     4046       4225   4770  Total Bags  \\\n",
       "0           NaN           NaN  1036.74   54454.85  48.16     8696.87   \n",
       "1           NaN           NaN      NaN   44638.81    NaN         NaN   \n",
       "2          0.93           NaN      NaN  109149.67    NaN     8145.35   \n",
       "3           NaN      78992.15  1132.00   71976.41    NaN         NaN   \n",
       "4           NaN      51039.60   941.48   43838.39  75.78     6183.95   \n",
       "\n",
       "   Small Bags  Large Bags  \n",
       "0         NaN         NaN  \n",
       "1         NaN         NaN  \n",
       "2         NaN         NaN  \n",
       "3      5677.4         NaN  \n",
       "4         NaN         NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will first make some holes in our data\n",
    "avocado_path = 'avocado.csv' #please make sure avacado.csv is in the same directory as the iPython notebook\n",
    "avocado_df = pd.read_csv(avocado_path,header=0)\n",
    "avocado_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "avocado_df.drop(avocado_df.columns[[0, 9, 10, 11, 12]], axis = 1, inplace = True) \n",
    "\n",
    "np.random.seed(100)\n",
    "mask = np.random.choice([True, False], size=avocado_df.shape)\n",
    "#print (mask)\n",
    "\n",
    "mask[mask.all(1),-1] = 0\n",
    "#print (mask)\n",
    "\n",
    "#print (avocado_df.mask(mask))\n",
    "\n",
    "#def add_random_na(row):\n",
    "#    vals = row.values\n",
    "#    for _ in range(random.randint(0,len(vals)-2)):\n",
    "#        i = random.randint(0,len(vals)-1)\n",
    "#        vals[i] = np.nan\n",
    "#    return vals\n",
    "\n",
    "avocado_df.mask(mask).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>1.63</td>\n",
       "      <td>17074.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13498.67</td>\n",
       "      <td>13066.82</td>\n",
       "      <td>431.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>1.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1191.70</td>\n",
       "      <td>3431.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8940.04</td>\n",
       "      <td>324.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>1.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>727.94</td>\n",
       "      <td>9394.11</td>\n",
       "      <td>9351.80</td>\n",
       "      <td>42.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>727.01</td>\n",
       "      <td>10969.54</td>\n",
       "      <td>10919.54</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2894.77</td>\n",
       "      <td>2356.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11988.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AveragePrice  Total Volume     4046     4225    4770  Total Bags  \\\n",
       "18244          1.63      17074.83      NaN      NaN     NaN    13498.67   \n",
       "18245          1.71           NaN  1191.70  3431.50     NaN         NaN   \n",
       "18246          1.87           NaN      NaN      NaN  727.94     9394.11   \n",
       "18247           NaN           NaN      NaN      NaN  727.01    10969.54   \n",
       "18248           NaN           NaN  2894.77  2356.13     NaN         NaN   \n",
       "\n",
       "       Small Bags  Large Bags  \n",
       "18244    13066.82      431.85  \n",
       "18245     8940.04      324.80  \n",
       "18246     9351.80       42.31  \n",
       "18247    10919.54         NaN  \n",
       "18248    11988.14         NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avocado_df.mask(mask).tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.40089245e+00 8.76004541e+05 1.03674000e+03 ... 8.69687000e+03\n",
      "  1.78906158e+05 5.54393126e+04]\n",
      " [1.40089245e+00 8.76004541e+05 2.90554507e+05 ... 2.42302277e+05\n",
      "  1.78906158e+05 5.54393126e+04]\n",
      " [9.30000000e-01 8.76004541e+05 2.90554507e+05 ... 8.14535000e+03\n",
      "  1.78906158e+05 5.54393126e+04]\n",
      " ...\n",
      " [1.87000000e+00 8.76004541e+05 2.90554507e+05 ... 9.39411000e+03\n",
      "  9.35180000e+03 4.23100000e+01]\n",
      " [1.40089245e+00 8.76004541e+05 2.90554507e+05 ... 1.09695400e+04\n",
      "  1.09195400e+04 5.54393126e+04]\n",
      " [1.40089245e+00 8.76004541e+05 2.89477000e+03 ... 2.42302277e+05\n",
      "  1.19881400e+04 5.54393126e+04]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# impute missing values - axix=0: impute along columns\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "print(imputer.fit_transform(avocado_df.mask(mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
